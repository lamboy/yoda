import os
import shutil
import tarfile
import requests
import numpy as np
from flask import Config
from celery import chord, shared_task
from sklearn.svm import SVC
from sklearn.grid_search import GridSearchCV
from sklearn.cross_validation import StratifiedKFold
from sklearn.metrics import accuracy_score
from sklearn.externals import joblib
from lib.util import generate_string, service_uri, timing_now, timing_elapsed_to_str
from lib.authentication import login_header, token_header
from lib.upload import upload_file
from util import load_features, get_xy

config = Config(None)
config.from_object('service.compute.settings')


# ----------------------------------------------------------------------------------------------------------------------
class Pipeline(object):

    def run(self, params):
        raise NotImplementedError()


# ----------------------------------------------------------------------------------------------------------------------
class ClassifierTrainingPipeline(Pipeline):
    """
    Trains a classifier and returns its ID as well as some additional meta-
    information about its accuracy.
    """
    def run(self, params):

        # Validate the pipeline parameters
        self.validate_params(params)
        # Request access token from auth service
        token = get_access_token()
        # Get file storage ID from storage service
        storage_id = get_file_storage_id(params['file_id'], token)
        # Columns to exclude (optional parameter)
        exclude_columns = []
        if 'exclude_columns' in params.keys():
            exclude_columns = params['exclude_columns']

        # Create sub-task for each fold in the cross-validation
        tasks = []
        for train, test in StratifiedKFold(params['subject_labels'], n_folds=params['nr_folds'], shuffle=True):
            # Convert train/test indices to regular lists otherwise they can't be serialized
            train, test = list(train), list(test)
            tasks.append(self.run_fold.subtask(
                (storage_id, train, test, params['index_column'], params['target_column'], exclude_columns, token)))

        # Create chord job that trains a classifier for each file ID and at the
        # end builds a task result object containing the output results.
        job = chord(header=tasks, body=self.build_task_result.subtask((
            storage_id, params['repository_id'], params['nr_folds'], params['index_column'],
            params['target_column'], exclude_columns, token)))
        result = job.apply_async()

        return result.task_id

    @staticmethod
    @shared_task
    def run_fold(storage_id, train, test, index_column, target_column, exclude_columns, token):

        # Create temporary folder for storing a local copy of the input file(s) as
        # well as any intermediate files that are generated by the pipeline.
        task_dir = create_task_dir()

        try:
            # Download the file to local task directory
            file_path = download_file(storage_id, task_dir, token)

            # Load features into memory
            print('Loading features')
            features = load_features(file_path, index_col=index_column)
            X, y = get_xy(features, target_column=target_column, exclude_columns=exclude_columns)

            # Define grid of hyper-parameter values to try
            param_grid = [{
                'C': [2 ** i for i in range(-5, 15, 2)],
                'gamma': [2 ** i for i in range(-15, 4, 2)]}]

            # Start training the classifier using a grid search approach for determining
            # the optimal hyper-parameters.
            print('Starting classifier training')
            start = timing_now()
            classifier = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid, scoring='accuracy')
            classifier.fit(X[train], y[train])
            y_pred = classifier.predict(X[test])
            y_true = y[test]

            # Calculate classification accuracy
            accuracy = accuracy_score(y_true, y_pred)

            # Log fold accuracy and time to complete
            print('Classifier accuracy {}'.format(accuracy))
            print('Fold finished after {}'.format(timing_elapsed_to_str(start)))

        finally:
            # Clean up task directory under any circumstances, even error
            delete_task_dir(task_dir)

        # Return the accuracy and optimal hyper-parameters of the classifier
        return {
            'accuracy': accuracy,
            'C': classifier.best_params_['C'],
            'gamma': classifier.best_params_['gamma']
        }

    @staticmethod
    @shared_task
    def build_task_result(outputs, storage_id, repository_id, nr_folds, index_column, target_column, exclude_columns, token):

        # Extract accuracies, C and gamma values from the outputs
        accuracies = []
        Cs = []
        gammas = []
        for i in range(len(outputs)):
            accuracies.append(outputs[i]['accuracy'])
            Cs.append(outputs[i]['C'])
            gammas.append(outputs[i]['gamma'])

        # Average the accuracies
        accuracy = sum(accuracies) / nr_folds
        print('Average accuracy: {}'.format(accuracy))

        # Figure out which hyper-parameters to choose
        max_accuracy = 0.0
        max_C = 0.0
        max_gamma = 0.0
        for i in range(len(accuracies)):
            if accuracies[i] > max_accuracy:
                max_accuracy = accuracies[i]
                max_C = Cs[i]
                max_gamma = gammas[i]
        print('Max. accuracy: {}, max. C: {}, max. gamma: {}'.format(max_accuracy, max_C, max_gamma))

        # Retrain classifier on all data using the optimal hyper-parameters
        task_dir = create_task_dir()

        try:
            # Download the file and load its features
            file_path = download_file(storage_id, task_dir, token)
            features = load_features(file_path, index_col=index_column)
            X, y = get_xy(features, target_column=target_column, exclude_columns=exclude_columns)

            # Train the classifier on all features with the optimal hyper-parameters
            print('Training classifier on all features')
            classifier = SVC(kernel='rbf', C=max_C, gamma=max_gamma)
            classifier.fit(X, y)

            # Save the classifier to disk and then upload it to storage service as regular file
            classifier_file_path = save_classifier(classifier, task_dir)
            classifier_id = upload_classifier_file(classifier_file_path, repository_id, token)

        finally:
            # Delete temporary task directory even though errors may have occured
            delete_task_dir(task_dir)

        # Return average accuracy and the optimal hyper-parameters
        return {
            'accuracy': accuracy,
            'C': max_C,
            'gamma': max_gamma,
            'classifier_id': classifier_id,
        }

    @staticmethod
    def validate_params(params):

        assert 'file_id' in params.keys()
        assert params['file_id'] > 0
        assert 'subject_labels' in params.keys()
        assert len(params['subject_labels']) > 0
        assert 'index_column' in params.keys()
        assert 'target_column' in params.keys()
        assert 'nr_folds' in params.keys()
        assert params['nr_folds'] > 0
        assert 'classifier' in params.keys()
        assert params['classifier'] in ['svm-lin', 'svm-rbf']
        assert 'repository_id' in params.keys()
        assert params['repository_id'] > 0


# ----------------------------------------------------------------------------------------------------------------------
class ClassificationPipeline(Pipeline):
    """
    Runs a trained classifier on one or more test observations. The classifier
    must have been trained previously via the ClassifierTrainingPipeline. This
    pipeline will have saved a classifier to disk.
    """
    def run(self, params):

        # Validate the pipeline parameters
        self.validate_params(params)
        # Request access token from auth service
        token = get_access_token()

        # Create temporary folder for storing a local copy of the input file(s) as
        # well as any intermediate files that are generated by the pipeline.
        task_dir = create_task_dir()

        try:
            # Download classifier. This will be a compressed .tar.gz archive containing
            # the different files saved by joblib.dump(). We first download the file from
            # the storage service, then unpack it and load the classifier object.
            classifier_file_path = download_file(params['classifier_id'], task_dir, token, extension='.tar.gz')
            classifier = load_classifier(classifier_file_path)
            # Setup Numpy array to hold the subject data
            X = np.array(params['subjects'])

            # Run classifier with the subject data to be predicted
            y_pred = classifier.predict(X)
            print('Classifier predicted {}'.format(y_pred))

        finally:
            # Delete task directory even if there were errors
            delete_task_dir(task_dir)

        # Return predictions for each subject
        return {'labels': list(y_pred)}

    @staticmethod
    def validate_params(params):

        assert 'classifier_id' in params.keys()
        assert 'subjects' in params.keys()
        assert len(params['subjects']) > 0


# ----------------------------------------------------------------------------------------------------------------------
def create_task_dir():
    task_dir = '/tmp/workers/task-{}'.format(generate_string())
    if os.path.isdir(task_dir):
        raise RuntimeError('Directory {} already exists'.format(task_dir))
    print('Creating directory {}'.format(task_dir))
    os.makedirs(task_dir)
    return task_dir


# ----------------------------------------------------------------------------------------------------------------------
def delete_task_dir(task_dir):
    if not os.path.isdir(task_dir):
        raise RuntimeError('Directory {} does not exist'.format(task_dir))
    print('Deleting directory {}'.format(task_dir))
    shutil.rmtree(task_dir)


# ----------------------------------------------------------------------------------------------------------------------
def get_access_token():
    print('Requesting access token')
    response = requests.post(
        '{}/tokens'.format(service_uri('auth')), headers=login_header(
            config['WORKER_USERNAME'],
            config['WORKER_PASSWORD']))
    return response.json()['token']


# ----------------------------------------------------------------------------------------------------------------------
def get_file_storage_id(file_id, token):
    print('Getting file storage ID')
    response = requests.get('{}/files/{}'.format(service_uri('storage'), file_id), headers=token_header(token))
    storage_id = response.json()['storage_id']
    print('File storage ID found: {}'.format(storage_id))
    return storage_id


# ----------------------------------------------------------------------------------------------------------------------
def download_file(storage_id, task_dir, token, extension=None):
    print('Downloading file with storage ID {}'.format(storage_id))
    response = requests.get(
        '{}/downloads/{}'.format(service_uri('storage'), storage_id), headers=token_header(token))
    file_path = os.path.join(task_dir, storage_id)
    if extension:
        if not extension.startswith('.'):
            extension = '.{}'.format(extension)
        file_path = '{}{}'.format(file_path, extension)
    with open(file_path, 'wb') as f:
        for chunk in response.iter_content(1024 * 1024):
            f.write(chunk)
    print('Downloaded file {}'.format(file_path))
    return file_path


# ----------------------------------------------------------------------------------------------------------------------
def save_classifier(classifier, task_dir):
    print('Saving classifier to compressed archive file')
    file_name = generate_string()
    file_path = os.path.join(task_dir, file_name)
    joblib.dump(classifier, file_path)
    arch_path = '{}.tar.gz'.format(file_path)
    arch_file = tarfile.open(arch_path, 'w:gz')
    for f in os.listdir(task_dir):
        if f.startswith(file_name):
            # We add each file based on its full file path but we specify the archive file
            # name to be just the file name (without its path). When we unpack the archive
            # later this will prevent subdirectories from being created.
            arch_file.add(os.path.join(task_dir, f), arcname=f)
            print('Added {}'.format(f))
    arch_file.close()
    print('Saved classifier {}'.format(arch_path))
    return arch_path


# ----------------------------------------------------------------------------------------------------------------------
def load_classifier(file_path):
    print('Loading classifier from compressed archive {}'.format(file_path))
    dir_name = os.path.join(os.path.dirname(file_path), 'classifier')
    arch_file = tarfile.open(file_path, 'r:gz')
    arch_file.extractall(dir_name)
    arch_file.close()
    # Search for main classifier file to use for loading joblib.load()
    main_file = None
    for f in os.listdir(dir_name):
        if not f.endswith('.npy'):
            main_file = os.path.join(dir_name, f)
            print('Found classifier main file {}'.format(main_file))
            break
    if main_file is None:
        raise RuntimeError('Could not find classifier main file')
    print('Loading classifier from file {}'.format(main_file))
    classifier = joblib.load(main_file)
    return classifier


# ----------------------------------------------------------------------------------------------------------------------
def upload_classifier_file(file_path, repository_id, token):
    print('Uploading classifier {}'.format(file_path))
    response = requests.get('{}/file-types?name=binary'.format(service_uri('storage')), headers=token_header(token))
    file_type_id = response.json()[0]['id']
    response = requests.get('{}/scan-types?name=none'.format(service_uri('storage')), headers=token_header(token))
    scan_type_id = response.json()[0]['id']
    try:
        _, storage_id = upload_file(file_path, file_type_id, scan_type_id, repository_id, token)
        return storage_id
    except RuntimeError as e:
        print('Failed to upload classifier ({})'.format(e.message))
