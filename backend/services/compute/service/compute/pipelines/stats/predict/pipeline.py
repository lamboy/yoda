import numpy as np
from lib.files import download_file
from service.compute.pipelines.base import Pipeline
from service.compute.pipelines.util import get_access_token, create_task_dir, delete_task_dir
from service.compute.pipelines.stats.util import load_model


# ----------------------------------------------------------------------------------------------------------------------
class ClassifierPredictionPipeline(Pipeline):

    def run(self, params):

        # Validate the pipeline parameters
        self.validate_params(params)
        # Request access token from auth service
        token = get_access_token()

        # Create temporary folder for storing a local copy of the input file(s) as
        # well as any intermediate files that are generated by the pipeline.
        task_dir = create_task_dir()

        try:
            # Download classifier. This will be a compressed .tar.gz archive containing
            # the different files saved by joblib.dump(). We first download the file from
            # the storage service, then unpack it and load the classifier object.
            classifier_file_path = download_file(params['classifier_id'], task_dir, token, extension='.tar.gz')
            classifier = load_model(classifier_file_path)
            # Setup Numpy array to hold the subject data
            X = np.array(params['subjects'])

            # Run classifier with the subject data to be predicted
            y_pred = classifier.predict(X)
            print('Classifier predicted {}'.format(y_pred))

        finally:
            # Delete task directory even if there were errors
            delete_task_dir(task_dir)

        # Return predictions for each subject
        return {'labels': list(y_pred)}

    @staticmethod
    def validate_params(params):
        assert 'classifier_id' in params.keys()
        assert 'subjects' in params.keys()
        assert len(params['subjects']) > 0
